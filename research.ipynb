{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b18890dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Downloading numpy-2.2.6-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-20.0.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.32.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in c:\\code\\machine learning - workspace\\text mining\\rag\\.venv\\lib\\site-packages (from datasets) (25.0)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.0-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.24.0->datasets)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.32.2->datasets)\n",
      "  Downloading charset_normalizer-3.4.2-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.32.2->datasets)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets)\n",
      "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: colorama in c:\\code\\machine learning - workspace\\text mining\\rag\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\code\\machine learning - workspace\\text mining\\rag\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.6.0-cp312-cp312-win_amd64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.4.4-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.1-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.0-cp312-cp312-win_amd64.whl.metadata (74 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\code\\machine learning - workspace\\text mining\\rag\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading huggingface_hub-0.32.0-py3-none-any.whl (509 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading numpy-2.2.6-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.6/12.6 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.2/12.6 MB 12.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 8.1/12.6 MB 13.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.3/12.6 MB 13.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 13.2 MB/s eta 0:00:00\n",
      "Downloading pyarrow-20.0.0-cp312-cp312-win_amd64.whl (25.7 MB)\n",
      "   ---------------------------------------- 0.0/25.7 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 2.9/25.7 MB 15.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 5.5/25.7 MB 13.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 8.1/25.7 MB 12.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 10.7/25.7 MB 12.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 13.4/25.7 MB 12.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 15.7/25.7 MB 12.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 18.4/25.7 MB 12.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 21.0/25.7 MB 12.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.6/25.7 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.7/25.7 MB 12.3 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.4/11.5 MB 12.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.0/11.5 MB 12.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.3/11.5 MB 11.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.4/11.5 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 11.2 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Downloading aiohttp-3.12.0-cp312-cp312-win_amd64.whl (435 kB)\n",
      "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp312-cp312-win_amd64.whl (105 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.6.0-cp312-cp312-win_amd64.whl (120 kB)\n",
      "Downloading multidict-6.4.4-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Downloading propcache-0.3.1-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Downloading yarl-1.20.0-cp312-cp312-win_amd64.whl (92 kB)\n",
      "Installing collected packages: pytz, xxhash, urllib3, tzdata, typing-extensions, tqdm, pyyaml, pyarrow, propcache, numpy, multidict, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, aiohappyeyeballs, yarl, requests, pandas, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.0 aiosignal-1.3.2 attrs-25.3.0 certifi-2025.4.26 charset-normalizer-3.4.2 datasets-3.6.0 dill-0.3.8 filelock-3.18.0 frozenlist-1.6.0 fsspec-2025.3.0 huggingface-hub-0.32.0 idna-3.10 multidict-6.4.4 multiprocess-0.70.16 numpy-2.2.6 pandas-2.2.3 propcache-0.3.1 pyarrow-20.0.0 pytz-2025.2 pyyaml-6.0.2 requests-2.32.3 tqdm-4.67.1 typing-extensions-4.13.2 tzdata-2025.2 urllib3-2.4.0 xxhash-3.5.0 yarl-1.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f176c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\ASUS\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\SEACrowd--nusax_mt\\5b94a49bda13fbbd67dde434228711a2e364d0e86bb99a8984d7a85949b1db6b (last modified on Sun May 25 22:28:52 2025) since it couldn't be found locally at SEACrowd/nusax_mt, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seacrowd'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSEACrowd/nusax_mt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Machine Learning - Workspace\\Text Mining\\RAG\\.venv\\Lib\\site-packages\\datasets\\load.py:2062\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2057\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   2058\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   2059\u001b[39m )\n\u001b[32m   2061\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2062\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   2080\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Machine Learning - Workspace\\Text Mining\\RAG\\.venv\\Lib\\site-packages\\datasets\\load.py:1817\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[39m\n\u001b[32m   1814\u001b[39m         error_msg += \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFor example `data_files=\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpath/to/data/train/*.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_extensions[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1815\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[32m-> \u001b[39m\u001b[32m1817\u001b[39m builder_cls = \u001b[43mget_dataset_builder_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1818\u001b[39m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[32m   1819\u001b[39m builder_instance: DatasetBuilder = builder_cls(\n\u001b[32m   1820\u001b[39m     cache_dir=cache_dir,\n\u001b[32m   1821\u001b[39m     dataset_name=dataset_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1831\u001b[39m     **config_kwargs,\n\u001b[32m   1832\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Machine Learning - Workspace\\Text Mining\\RAG\\.venv\\Lib\\site-packages\\datasets\\load.py:249\u001b[39m, in \u001b[36mget_dataset_builder_class\u001b[39m\u001b[34m(dataset_module, dataset_name)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_dataset_builder_class\u001b[39m(\n\u001b[32m    242\u001b[39m     dataset_module: \u001b[33m\"\u001b[39m\u001b[33mDatasetModule\u001b[39m\u001b[33m\"\u001b[39m, dataset_name: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    243\u001b[39m ) -> \u001b[38;5;28mtype\u001b[39m[DatasetBuilder]:\n\u001b[32m    244\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m    245\u001b[39m         lock_importable_file(dataset_module.importable_file_path)\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m dataset_module.importable_file_path\n\u001b[32m    247\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m nullcontext()\n\u001b[32m    248\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m         builder_cls = \u001b[43mimport_main_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dataset_module.builder_configs_parameters.builder_configs:\n\u001b[32m    251\u001b[39m         dataset_name = dataset_name \u001b[38;5;129;01mor\u001b[39;00m dataset_module.builder_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mdataset_name\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Machine Learning - Workspace\\Text Mining\\RAG\\.venv\\Lib\\site-packages\\datasets\\load.py:168\u001b[39m, in \u001b[36mimport_main_class\u001b[39m\u001b[34m(module_path)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimport_main_class\u001b[39m(module_path) -> Optional[\u001b[38;5;28mtype\u001b[39m[DatasetBuilder]]:\n\u001b[32m    167\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Import a module at module_path and return its main class: a DatasetBuilder\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# Find the main class in our imported module\u001b[39;00m\n\u001b[32m    170\u001b[39m     module_main_cls = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:999\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\SEACrowd--nusax_mt\\5b94a49bda13fbbd67dde434228711a2e364d0e86bb99a8984d7a85949b1db6b\\nusax_mt.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseacrowd\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m schemas\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseacrowd\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfigs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SEACrowdConfig\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseacrowd\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (DEFAULT_SEACROWD_VIEW_NAME,\n\u001b[32m     10\u001b[39m                                        DEFAULT_SOURCE_VIEW_NAME, Tasks)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seacrowd'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dset = load_dataset(\"SEACrowd/nusax_mt\", trust_remote_code=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
